{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM et Sélection d'attribut\n",
    "\n",
    "\n",
    "\n",
    "## Variables d'environement\n",
    "\n",
    "Pensez à vérifier les variables d'environement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation linéaire\n",
    "\n",
    "Le but de cette partie est de comparer le SVM linéaire à un autre exemple de classifieur linéaire: le Perceptron. On commence d'abords par rappeler rapidement le principe du Perceptron.\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "L'algorithm du Perceptron date des [travaux de Frank Rosenblatt](http://psycnet.apa.org/record/1959-09865-001). Le but était de modéliser l'action des neurones. Ce modèle va être ensuite utilisé pour contruire des réseaux de neurones complexes et c'est la base de toute les méthodes de Deep Learning. Le modèle donne pour chaque attribut $i \\in \\{1,2, \\dots,d\\}$ de la donnée d'entrée $x = \\begin{pmatrix}x_1\\\\ x_2\\\\ \\vdots \\\\x_d\\end{pmatrix}$ un poids $w_i$. Pour chaque entrée $x$ on lui applique linéairement un vecteur de poids $w = \\begin{pmatrix}w_1\\\\ w_2\\\\ \\vdots \\\\w_d\\end{pmatrix}$ pour lui attribuer un score $s = \\langle w \\vert x\\rangle = \\sum_{i=1,\\dots,d}w_i.x_i$. Suite à ce score obtenu, on prends une décision:\n",
    "* si $s < c \\in \\mathbb{R}$, on choisit la classe $0$;\n",
    "* si $s \\geq c $, on choisit la classe $1$\n",
    "\n",
    "On peut écrire donc ce classifieur autrement:\n",
    "\n",
    "$$D_{perceptron}(x) \\triangleq \\mathbb{1}_{\\langle w \\vert x\\rangle + b \\geq 0}$$\n",
    "\n",
    "où $b = -c$ et $\\mathbb{1}_A(x) = \\begin{cases}1 & , x \\in A\\\\0 & , x \\notin A\\end{cases}$.\n",
    "\n",
    "Si on cherche à rammener les classes à la convention SVM (i.e. $y=\\pm1$), avec une simple transformation affine, on a:\n",
    "$$\\widetilde{D}_{perceptron}(x) \\triangleq 2.\\mathbb{1}_{\\langle w \\vert x \\rangle + b \\geq 0} - 1 = sign(\\langle w \\vert x \\rangle + b \\geq 0)$$\n",
    "\n",
    "### Régression logistique\n",
    "\n",
    "Le modèle de régression logistique est proche des méthodes génératives. Ce Modèle permet juste de donner une relation entre les probabilités par classe et non pas les distributions en elle même:\n",
    "$$ \\ln \\Big( \\frac{p(x \\vert y=1)}{p(x \\vert y=0)}\\Big) = \\langle w \\vert x \\rangle + b$$\n",
    "\n",
    "1. \n",
    "    a. En appliquant la règle de Bayes, montrer que: $$\\ln\\Big(\\frac  {p(y=1\\vert x)}{1-p(y=1\\vert x)}\\Big) = \\ln\\Big(\\frac{p(y=1)}{p(y=0)}\\Big) + b +\\langle w \\vert x \\rangle$$\n",
    "    b. Montrer donc que le décideur de la régression logistique est:\n",
    "$$D_{logistic} = \\sigma(\\tilde b +\\langle w \\vert x \\rangle)$$\n",
    "où: $\\sigma(t) \\triangleq \\frac{1}{1 + e^{-t}} \\quad ,\\forall t \\in \\mathbb{R}$\n",
    "\n",
    "2. Ecrire un code python qui trace les deux fonctions, avec de multiple valeurs de $\\lambda$, $t \\mapsto \\sigma(\\lambda.t)$ et $t \\mapsto \\mathbb{1}_{t \\geq 0}$, dans une même figure. A la lumière de la figure obtenue, discuter les deux fonctions de décisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réponse\n",
    "\n",
    "1. a. \n",
    "\n",
    "   b. \n",
    "\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(-20, 20, 1000)\n",
    "\n",
    "lambdas = [.1, 1, 10]\n",
    "colors = ['k', 'b', 'r']\n",
    "\n",
    "# Utiliser 'plt' pour tracer les courbes qui correspondent à \\sigma avec les lambda données et la fonctions Heavyside\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison\n",
    "\n",
    "On rappelle ici que le SVM linéaire a pour but de maximiser la marge entre deux classes, contrairement au Perceptron et à la régression logistique. Les problèmes à optimiser ne se ressemble plus.\n",
    "\n",
    "Le but du code, ci-dessous, est d'illustrer cette différence.\n",
    "\n",
    "1. a. Qu'est ce que fait ce bout de code?\n",
    "\n",
    "   b. Commentez le résultat du programme suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "\n",
    "def plot_points(points, ax, color):\n",
    "    ax.scatter(points[:, 0], points[:, 1], c=color)\n",
    "    \n",
    "\n",
    "def plot_dataset(X, Y, ax):\n",
    "    for x, col in zip([X[Y==0], X[Y==1]], ['r', 'b']):\n",
    "        plot_points(x, ax, col)\n",
    "        \n",
    "\n",
    "def mesh_from(instances, gap=.2):\n",
    "    return np.meshgrid(\n",
    "        np.arange(X[:, 0].min() - 1, X[:, 0].max() + 1, gap),\n",
    "        np.arange(X[:, 1].min() - 1, X[:, 1].max() + 1, gap),\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_contours(xx, yy, ax, classifier, **parameters):\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, **parameters)\n",
    "\n",
    "\n",
    "def plot_margin(xx, yy, ax, classifier, **parameters):\n",
    "    Z = np.empty(xx.shape)\n",
    "    for (i, j), value in np.ndenumerate(xx):\n",
    "        Z[i, j] = classifier.decision_function([[value, yy[i, j]]])[0]\n",
    "    ax.contour(xx, yy, Z, [-1.0, 0.0, 1.0], colors='k', linestyles=['dashed', 'solid', 'dashed'])\n",
    "\n",
    "\n",
    "X, Y = sklearn.datasets.make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.60)\n",
    "xx, yy = mesh_from(X, .01)\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True)\n",
    "f.set_figheight(5)\n",
    "f.set_figwidth(15)\n",
    "\n",
    "for ax, loss , title in zip([ax1, ax2, ax3], ['hinge', 'perceptron', 'log'], ['SVM', 'Perceptron', 'Logistic Regression']):\n",
    "    plot_dataset(X, Y, ax)\n",
    "    model = sklearn.linear_model.SGDClassifier(alpha=0.01, max_iter=100, loss=loss).fit(X, Y)\n",
    "    plot_contours(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        model,\n",
    "        cmap=plt.cm.viridis,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    plot_margin(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        model\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réponse:\n",
    "\n",
    "1. a. \n",
    "\n",
    "   b. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pénalisation vs Généralisation\n",
    "\n",
    "1. a. Entraîner des SVM linéaire avec différentes constantes de pénalisation $C$ sur les mêmes données.\n",
    "\n",
    "   b. Tracer la marge selon les valeurs de la constante $C$.\n",
    "   \n",
    "   c. Commenter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm\n",
    "\n",
    "C = [.01, .1, 1, 10, 100, 1000]\n",
    "\n",
    "f, axes = plt.subplots(1, len(C), sharey=True)\n",
    "f.set_figheight(5)\n",
    "f.set_figwidth(30)\n",
    "\n",
    "for c, ax in zip(C, list(axes)):\n",
    "    plot_dataset(X, Y, ax)\n",
    "    # Entraîner le modèle\n",
    "    # Tracer les contours et la marge\n",
    "    ax.set_title('C = ' + str(c))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réponse:\n",
    "\n",
    "1. a. \n",
    "\n",
    "   b. \n",
    "   \n",
    "   c.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVM\n",
    "\n",
    "1. a. Relancer le même code mais cette fois-ci avec le kernel polynomial et le kernel rbf en jouant sur le $gamma$ sur les données suivantes.\n",
    "\n",
    "   b. Commenter les résultats.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_circles(n_samples=100, factor=.3, noise=.05)\n",
    "xx, yy = mesh_from(X, .01)\n",
    "\n",
    "gammas = [.5, 1, 2, 3, 4]\n",
    "\n",
    "f, axes = plt.subplots(2, len(gammas), sharey=True)\n",
    "f.set_figheight(10)\n",
    "f.set_figwidth(25)\n",
    "\n",
    "for gamma, ax in zip(gammas, list(axes[0, :])):\n",
    "    plot_dataset(X, Y, ax)\n",
    "    # Entraîner le modèle\n",
    "    # Tracer les contours et la marge\n",
    "    ax.set_title('pol, $\\gamma$ = ' + str(gamma))\n",
    "\n",
    "gammas = [.01, .1, 2, 10, 100]\n",
    "for gamma, ax in zip(gammas, list(axes[1, :])):\n",
    "    plot_dataset(X, Y, ax)\n",
    "    # Entraîner le modèle\n",
    "    # Tracer les contours et la marge\n",
    "    ax.set_title('rbf, $\\gamma$ = ' + str(gamma))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réponse:\n",
    "\n",
    "1. a. \n",
    "\n",
    "   b. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM vs Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection d'attribut\n",
    "\n",
    "### OCS\n",
    "\n",
    "### Selectionner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
