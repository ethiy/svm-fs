{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM et Sélection d'attribut\n",
    "\n",
    "\n",
    "\n",
    "## Variables d'environement\n",
    "\n",
    "Pensez à vérifier les variables d'environement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation linéaire\n",
    "\n",
    "Le but de cette partie est de comparer le SVM linéaire à un autre exemple de classifieur linéaire: le Perceptron. On commence d'abords par rappeler rapidement le principe du Perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "L'algorithm du Perceptron date des [travaux de Frank Rosenblatt](http://psycnet.apa.org/record/1959-09865-001). Le but était de modéliser l'action des neurones. Ce modèle va être ensuite utilisé pour contruire des réseaux de neurones complexes et c'est la base de toute les méthodes de Deep Learning.\n",
    "\n",
    "Le modèle donne pour chaque stimulus $i \\in \\{1,2, \\dots,d\\}$ de la donnée d'entrée $x = \\begin{pmatrix}x_1\\\\ x_2\\\\ \\vdots \\\\x_d\\end{pmatrix}$ un poids d'importance $w_i$. On combine ses différent stimuli $x$ en pondérant leur somme par le vecteur de poids $w = \\begin{pmatrix}w_1\\\\ w_2\\\\ \\vdots \\\\w_d\\end{pmatrix}$ pour calculer un score $s = \\langle w \\vert x\\rangle = \\sum_{i=1,\\dots,d}w_i.x_i$. Suite à ce score obtenu, on prends une décision:\n",
    "* si $s < c \\in \\mathbb{R}$, on choisit la classe $0$;\n",
    "* si $s \\geq c $, on choisit la classe $1$\n",
    "\n",
    "On peut écrire donc la fonction de décision du perceptron comme:\n",
    "\n",
    "$$D_{perceptron}(x) \\triangleq 2.\\mathbb{1}_{\\langle w \\vert x \\rangle + b \\geq 0} - 1 = sign(\\langle w \\vert x \\rangle + b)\\quad , \\forall x \\in \\mathbb{R}^d$$\n",
    "\n",
    "où $b = -c$ et $\\mathbb{1}_A(x) = \\begin{cases}1 & , x \\in A\\\\0 & , x \\notin A\\end{cases}\\quad , \\forall x \\in \\mathbb{R}^d$.\n",
    "\n",
    "Le modèle du Perceptron revient donc à choisir un modèle de séparation de donnée linéaire. C'est en réalité une fammille de séparateur possibles. Nous n'avons aucune garantie sur son pouvoir de généralisation.\n",
    "\n",
    "1. Qu'est ce qui différencie le SVM par rapport au Perceptron en terme de pouvoir de généralisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réponse\n",
    "\n",
    "1.\n",
    "Le SVM est modèle de séparateur linéaire comme le Perceptron. La différence réside dans le faite que le SVM cherche à maximiser la marge - qui représente l'écart entre les deux classes - dans le but d'avoir un meilleur pouvoir de généralisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression logistique\n",
    "\n",
    "Le modèle de régression logistique est proche des méthodes génératives. Ce modèle donne une relation entre les probabilités des attributs sachant la classe, comme suit:\n",
    "$$ \\ln \\Big( \\frac{p(x \\vert y=1)}{p(x \\vert y=-1)}\\Big) = \\langle w \\vert x \\rangle + c \\quad , \\forall x \\in \\mathbb{R}^d$$\n",
    "\n",
    "2.\n",
    "    a. En appliquant la règle de Bayes, montrer que: \n",
    "    $$\\frac  {p(y=1\\vert x)}{p(y=-1\\vert x)} = \\frac{p(y=1)}{p(y=-1)} . \\frac{p(x \\vert y=1)}{p(x \\vert y=-1)}\\quad , \\forall x \\in \\mathbb{R}^d $$\n",
    "    b. En déduire la formule suivante:\n",
    "    $$\\ln\\Big(\\frac  {p(y=1\\vert x)}{1-p(y=1\\vert x)}\\Big) = \\ln\\Big(\\frac{p(y=1)}{p(y=-1)}\\Big) + c +\\langle w \\vert x \\rangle \\quad , \\forall x \\in \\mathbb{R}^d $$\n",
    "    c. On rappelle que $\\eta(x) = p(y=1\\vert x) , \\forall x \\in \\mathbb{R}^d$. Montrer que:\n",
    "$$\\eta(x) = \\sigma( b +\\langle w \\vert x \\rangle)\\quad , \\forall x \\in \\mathbb{R}^d$$\n",
    "où: $$\\sigma(t) \\triangleq \\frac{1}{1 + e^{-t}} \\quad ,\\forall t \\in \\mathbb{R}$$\n",
    "    d. Montrer que:\n",
    "    $$ sign(\\sigma(t) - \\frac{1}{2}) = sign(t)\\quad ,\\forall t \\in \\mathbb{R}$$\n",
    "    On rappelle que:\n",
    "    $$\\forall x \\in \\mathbb{R} \\quad sign(x) = 2.\\mathbb{1}_{x\\geq0} - 1 = 2. \\begin{cases}1 & , x\\geq0 \\\\ 0 &, x < 0 \\end{cases} - 1 = \\begin{cases}1 & , x \\geq 0 \\\\ -1 &, x < 0 \\end{cases}$$\n",
    "    $$$$\n",
    "    $$$$\n",
    "    On fixe $x \\in \\mathbb{R}^d$, et on rappelle que, dans le cas binaire, la classe la plus probable $\\hat y = \\arg \\max_y p(y\\vert x)$ vérifie: \n",
    "    $$ p(\\hat y \\vert x) \\geq \\frac{1}{2} $$\n",
    "    En effet, puisque $y$ prend deux valeurs $1$ et $-1$, on peut écrire:\n",
    "    $$p(y \\vert x) + p(-y \\vert x) = 1$$\n",
    "    donc:\n",
    "    $$p(y \\vert x) \\geq \\frac{1}{2} \\Rightarrow p(-y \\vert x) = 1 - p(y \\vert x) \\leq 1 - \\frac{1}{2} = \\frac{1}{2} \\leq p(y \\vert x) \\Rightarrow p(y \\vert x) \\geq \\frac{1}{2} $$\n",
    "    Inversement, par définition:\n",
    "    $$p(\\hat y \\vert x) \\geq p( -\\hat y \\vert x) = 1 - p(\\hat y \\vert x) \\Rightarrow 2 . p(\\hat y \\vert x) \\geq 1 \\Rightarrow  p(\\hat y \\vert x) \\geq \\frac{1}{2} $$\n",
    "    On verifie donc bien que: \n",
    "    $$p(y \\vert x) \\geq \\frac{1}{2} \\Leftrightarrow y = \\hat y = \\arg \\max_y p(y \\vert x)$$\n",
    "    e. En déduire que la fonction de décision de Bayes pour le modèle logistique vérifie:\n",
    "    $$ D_{logistic} = sign(\\sigma( b +\\langle w \\vert \\cdot \\rangle) - \\frac{1}{2}) = sign(b +\\langle w \\vert \\cdot\n",
    "    \\rangle) = D_{perceptron}$$    \n",
    "    f. Qu'est ce que vérifie le séparateur dans un modèle logistique?\n",
    "\n",
    "3.\n",
    "    Ecrire un code python qui trace les deux fonctions, avec de multiple valeurs de $\\lambda$, $t \\mapsto \\sigma(\\lambda.t)$ et $t \\mapsto \\mathbb{1}_{t \\geq 0}$, dans une même figure. A la lumière de la figure obtenue, discuter les deux fonctions de décisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réponse\n",
    "\n",
    "2.\n",
    "a. On fixe $x \\in \\mathbb{R}^d$ et $y \\in \\{-1, 1\\}$. On sait que:\n",
    "\n",
    "$$ p(x, y) = p(x \\vert y) . p(y) = p(y \\vert x) . p(x)$$\n",
    "\n",
    "On en déduit la règle de Bayes:\n",
    "\n",
    "$$ p(y\\vert x) = \\frac{p(x \\vert y) . p(y)}{p(x)}$$\n",
    "\n",
    "Donc pour $x \\in \\mathbb{R}^d$:\n",
    "\n",
    "$$\\frac{p(y=1 \\vert x)}{p(y = -1 \\vert x)} = \\frac{p(x \\vert y=1).p(y=1)}{p(x)} . \\frac{p(x)}{p(x\\vert y = -1) . p(y=-1)}$$\n",
    "\n",
    "$$\\frac{p(y=1 \\vert x)}{p(y = -1 \\vert x)} = \\frac{p(x \\vert y=1).p(y=1)} {p(x\\vert y = -1) . p(y=-1)} = \\frac{p(y=1)}{p(y=-1)} . \\frac{p(x \\vert y=1)}{p(x \\vert y=-1)}$$\n",
    "\n",
    "b.\n",
    "Soit $x \\in \\mathbb{R}^d$. On commence par remarquer que:\n",
    "$$p(y=1 \\vert x) + p(y=-1 \\vert x) = 1 \\Rightarrow p(y=-1 \\vert x) = 1 - p(y=1 \\vert x) $$\n",
    "On remplace donc $p(y=-1 \\vert x)$ dans le résultat précédent:\n",
    "\n",
    "$$\\frac{p(y=1 \\vert x)}{1 - p(y = 1 \\vert x)} = \\frac{p(y=1)}{p(y=-1)} . \\frac{p(x \\vert y=1)}{p(x \\vert y=-1)}$$\n",
    "\n",
    "On introduit le logarithme sur les deux côtés de cette équation:\n",
    "\n",
    "$$\\ln \\Big(\\frac{p(y=1 \\vert x)}{1 - p(y = 1 \\vert x)}\\Big) = \\ln \\Big(\\frac{p(y=1)}{p(y=-1)}\\Big) + \\ln \\Big(\\frac{p(x\\vert y=1)}{p(x\\vert y=-1)}\\Big)$$\n",
    "$$\\ln \\Big(\\frac{p(y=1 \\vert x)}{1 - p(y = 1 \\vert x)}\\Big) = \\ln \\Big(\\frac{p(y=1)}{p(y=-1)}\\Big) + \\langle w \\vert x \\rangle + c $$\n",
    "\n",
    "c.\n",
    "Soit $x \\in \\mathbb{R}^d$, d'après la question précédente:\n",
    "$$\\ln \\Big(\\frac{\\eta(x)}{1 - \\eta(x)}\\Big) = \\ln \\Big(\\frac{p(y=1)}{p(y=-1)}\\Big) + \\langle w \\vert x \\rangle + c $$\n",
    "On pose $b = \\ln \\Big(\\frac{p(y=1)}{p(y=-1)}\\Big) + c$\n",
    "On obtient:\n",
    "$$\\ln \\Big(\\frac{\\eta(x)}{1 - \\eta(x)}\\Big) = \\langle w \\vert x \\rangle + b $$\n",
    "Donc:\n",
    "$$\\frac{\\eta(x)}{1 - \\eta(x)} = e^{\\langle w \\vert x \\rangle + b} \\Leftrightarrow \\eta(x) = (1 - \\eta(x)).e^{\\langle w \\vert x \\rangle + b}$$\n",
    "$$\\Leftrightarrow (1+e^{\\langle w \\vert x \\rangle + b}). \\eta(x) = e^{\\langle w \\vert x \\rangle + b}$$\n",
    "$$\\Leftrightarrow \\eta(x) = \\frac{e^{\\langle w \\vert x \\rangle + b}}{1+e^{\\langle w \\vert x \\rangle + b}}$$\n",
    "$$\\Leftrightarrow \\eta(x) = \\frac{1}{1+e^{-(\\langle w \\vert x \\rangle + b)}}$$\n",
    "d'où:\n",
    "$$\\eta(x) = \\sigma(\\langle w \\vert x \\rangle + b)\\quad, \\forall x \\in \\mathbb{R}^d$$\n",
    "\n",
    "d.\n",
    "$$\\forall t \\in \\mathbb{R} \\quad \\sigma(t) - \\frac{1}{2} = \\frac{1}{1 + e^{-t}} - \\frac{1}{2} = \\frac{2 - (1 + e^{-t})}{2.(1 + e^{-t})} = \\frac{1-e^{-t}}{2.(1 + e^{-t})} $$\n",
    "On sait que pour tout $t \\in \\mathbb{R}$ :\n",
    "$$e^{-t} > 0 \\Rightarrow 2.(1 + e^{-t}) > 0 \\Rightarrow sign(2.(1 + e^{-t})) = 1 $$\n",
    "Donc:\n",
    "$$sign(\\sigma(t) - \\frac{1}{2}) = sign(1-e^{-t})$$\n",
    "$t \\mapsto e^t$ est croissante donc, $t \\mapsto 1-e^{-t} $ l'est aussi:\n",
    "$$ 1-e^{-t} \\geq 1-e^0 = 0 , \\quad \\forall t \\geq 0$$ et inversement:\n",
    "$$ 1-e^{-t} < 1-e^0 = 0 , \\quad \\forall t < 0$$\n",
    "\n",
    "on conclut que:\n",
    "$$sign(\\sigma(t) - \\frac{1}{2}) = sign(1-e^{-t}) = sign(t)$$\n",
    "$$$$\n",
    "e.\n",
    "On a calculé:\n",
    "$$p(y=1 \\vert x) = \\sigma(\\langle w \\vert x \\rangle + b) \\quad \\forall x \\in \\mathbb{R}^d$$\n",
    "La fonction de décision pour la régression logistique $D_{logistique}$ vérifie, pour un $x$ donné:\n",
    "$$D_{logistique}(x) = \\begin{cases}1 & ,p(y = 1 \\vert x) \\geq \\frac{1}{2} \\\\ -1 & ,p(y = 1 \\vert x) < \\frac{1}{2} \\end{cases} = sign(p(y = 1 \\vert x))\\overset{\\mbox{(c.)}}{=} sign(\\sigma(\\langle w \\vert x \\rangle + b)) \\overset{\\mbox{(d.)}}{=} sign(\\langle w \\vert x \\rangle + b) = D_{perceptron}(x)$$\n",
    "\n",
    "f.\n",
    "Puisque $ D_{logistique} = D_{perceptron} $, la régression logistique implique un séparateur linéaire.\n",
    "\n",
    "3.\n",
    "Au fur et à mesure que le facteur d'échelle $\\lambda$ grandit $t \\mapsto \\sigma(\\lambda. t)$ converge vers la fonction Heavyside $\\mathbb{1}_{t\\geq 0}$. La régression logistique peut donc être vu comme une relaxation du Perceptron. Dans le cas des réseaux de neurones, ces fonctions s'appellent des fonctions d'activations. l'intérêt de la sigmoide $\\sigma$ est d'approcher la Heavyside tout en étant dérivable. Cela permet de faciliter le problème d'optimisation qui cherche les droites de séparation optimale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(-20, 20, 1000)\n",
    "\n",
    "lambdas = [.1, 1, 10]\n",
    "colors = ['k', 'b', 'r']\n",
    "\n",
    "for (l, c) in zip(lambdas, colors):\n",
    "    plt.plot(x, 1/(1 + np.exp(-l*x)), '--', color=c, label='$\\sigma(\\lambda.t)$, with $\\lambda = {}$'.format(l))\n",
    "\n",
    "plt.plot(x, x>0, 'y-', label='Heavyside, $\\mathbb{1}_{t\\geq 0}$')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaison\n",
    "\n",
    "Le but du code, ci-dessous, est d'illustrer la différence entre le SVM, le Perceptron et la régression logistique.\n",
    "\n",
    "4.\n",
    "   a. Qu'est ce que fait ce bout de code?\n",
    "\n",
    "   b. Commentez le résultat du programme suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "\n",
    "def plot_points(points, ax, color):\n",
    "    ax.scatter(points[:, 0], points[:, 1], c=color)\n",
    "    \n",
    "\n",
    "def plot_dataset(X, Y, ax, colors=['r', 'b']):\n",
    "    for x, col in zip([X[Y==0], X[Y==1]], colors):\n",
    "        plot_points(x, ax, col)\n",
    "        \n",
    "\n",
    "def mesh_from(instances, gap=.2):\n",
    "    return np.meshgrid(\n",
    "        np.arange(X[:, 0].min() - 1, X[:, 0].max() + 1, gap),\n",
    "        np.arange(X[:, 1].min() - 1, X[:, 1].max() + 1, gap),\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_separator(xx, yy, ax, classifier, **parameters):\n",
    "    \"\"\"\n",
    "        Plots separator.\n",
    "        \n",
    "        xx: mesh first coordinates\n",
    "        yy: mesh second coordinates\n",
    "        ax: subplot to draw in\n",
    "        classifier: the trained classifier\n",
    "    \"\"\"\n",
    "    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, **parameters)\n",
    "\n",
    "\n",
    "def plot_margin(xx, yy, ax, classifier, **parameters):\n",
    "    \"\"\"\n",
    "        Plots margins.\n",
    "        \n",
    "        xx: mesh first coordinates\n",
    "        yy: mesh second coordinates\n",
    "        ax: subplot to draw in\n",
    "        classifier: the trained classifier\n",
    "    \"\"\"\n",
    "    Z = np.empty(xx.shape)\n",
    "    for (i, j), value in np.ndenumerate(xx):\n",
    "        Z[i, j] = classifier.decision_function([[value, yy[i, j]]])[0]\n",
    "    ax.contour(xx, yy, Z, [-1.0, 0.0, 1.0], colors='k', linestyles=['dashed', 'solid', 'dashed'])\n",
    "\n",
    "\n",
    "X, Y = sklearn.datasets.make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.60)\n",
    "xx, yy = mesh_from(X, .01)\n",
    "\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True)\n",
    "f.set_figheight(5)\n",
    "f.set_figwidth(15)\n",
    "\n",
    "for ax, loss , title in zip([ax1, ax2, ax3], ['hinge', 'perceptron', 'log'], ['SVM', 'Perceptron', 'Logistic Regression']):\n",
    "    plot_dataset(X, Y, ax)\n",
    "    model = sklearn.linear_model.SGDClassifier(alpha=0.01, max_iter=100, loss=loss).fit(X, Y)\n",
    "    plot_separator(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        model,\n",
    "        cmap=plt.cm.viridis,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    plot_margin(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        model\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réponse:\n",
    "\n",
    "4.\n",
    "b.\n",
    "Le Perceptron accepte beaucoup de solution qui dépend de l'initialisation et l'algorithme utilisé. La solution trouvée ici privilègie la classe bleue. En effet, parmis les points rouges - qui sont supposés être sûrs comme les instances bleues - on trouve quelque-uns qui se retrouve dans la marge, qui est censée contenir des points non sûrs.\n",
    "\n",
    "La régression logistique permet de pallier à ce problème. Cependant, on remarque que la marge est étroite. Le SVM, quant à lui, cherche la séparation qui garanti la plus grande marge (et donc le plus de pouvoir de généralisation) en restant fidèle au données. On voit bien la différence entre SVM et la régression logistique. La marge obtenue grâce au SVM est, en effet, plus grande. Le SVM qui agit avec des hypothèses plus faible que la régression logistique donne de meilleur résultat que la régression logistique qui est plus contraignante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pénalisation vs Généralisation\n",
    "\n",
    "5.\n",
    "   a. Entraîner des SVM linéaire avec différentes constantes de pénalisation $C$ sur les mêmes données.\n",
    "\n",
    "   b. Tracer la marge selon les valeurs de la constante $C$.\n",
    "   \n",
    "   c. Commenter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm\n",
    "\n",
    "Cs = [.01, .1, .5, 1, 10, 100, 1000]\n",
    "\n",
    "f, axes = plt.subplots(1, len(Cs), sharey=True)\n",
    "f.set_figheight(5)\n",
    "f.set_figwidth(5 * len(Cs))\n",
    "\n",
    "for C, ax in zip(Cs, list(axes)):\n",
    "    plot_dataset(X, Y, ax)\n",
    "    model = sklearn.svm.SVC(C=C, kernel='linear').fit(X, Y)\n",
    "    plot_separator(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        model,\n",
    "        cmap=plt.cm.viridis,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    plot_margin(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        model\n",
    "    )\n",
    "    ax.set_title('C = ' + str(C))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réponse:\n",
    "\n",
    "1.\n",
    "c.\n",
    "On rappelle le problème d'optimisation du SVM:\n",
    "\n",
    "$$ \\begin{align}& \\min_{\\textbf{w}} & & {\\vert\\vert \\textbf{w} \\vert\\vert}^2 \\\\\n",
    "         & \\text{sous contrainte}& & Y^i.(\\textbf{w}.\\textbf{X}^i + b) \\geq 1 \\; \\forall i = 1, \\dots, n.\\end{align}$$\n",
    "\n",
    "Ce problème correspond à ce que l'on appelle la marge \"dure\" (Hard Margin). On impose que tout les points d'entraînement soient à l'extérieur de la marge. On peut relacher ce problème en SVM avec marge souple (Soft Margin). On agrandit la marge pour avoir plus de généralisation au détriment de la certitude des points donnés. Le problème devient donc: \n",
    "$$\\begin{align}& \\min_{\\textbf{w}, \\xi_1,\\dots,\\xi_n \\in \\mathbb{R}^+}& & {\\vert\\vert \\textbf{w} \\vert\\vert}^2 + C.\\sum_{i=1,\\dots,n}\\xi_i\\\\\n",
    "         & \\text{sous contrainte}& & Y^i.(\\textbf{w}.\\textbf{X}^i + b) \\geq 1 - \\xi_i , \\forall i = 1, \\dots, n.\\end{align}$$\n",
    "         \n",
    "On rajoute le terme $C.\\sum_{i=1,\\dots,n}\\xi_i$ pour pénaliser les variables ressorts qui indiquent, pour chaque point  de la données, sa distances de la marge. Ces variables sont positives pour ne pénaliser que les instances qui sont en dehors de la marge. On peut donc voir grâce aux expériences que, plus on augmente le $C$, plus la marge se rétrécit. A partir d'un seuil $C\\geq 1$, la pénalisation n'impacte plus le résultat. Cela correspond, en réalité au cas où $C = \\infty $ pour notre configuration. On tombe donc sur le premier problème d'optimisation. On risque dans ce cas, de perdre du pourvoir de généralisation et de tomber dans du sur-apprentissage. Inversement, si $C$ est trop  petit, on changerait trop l'orientation du séparateur pour accomoder plus d'erreurs: on risque donc le sous-apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVM\n",
    "\n",
    "6.\n",
    "   a. Entraîner le SVM, en choisissant la meilleur valeur pour $C$, avec le kernel polynomial et le kernel rbf en jouant sur le $\\gamma$ sur les données suivantes.\n",
    "\n",
    "   b. Commenter les résultats.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_circles(n_samples=100, factor=.3, noise=.05)\n",
    "xx, yy = mesh_from(X, .01)\n",
    "\n",
    "gammas = [1, 2, 3, 4, 5, 6]\n",
    "f, axes = plt.subplots(2, len(gammas), sharey=True)\n",
    "f.set_figheight(20)\n",
    "f.set_figwidth(10*len(gammas))\n",
    "\n",
    "for gamma, ax in zip(gammas, list(axes[0, :])):\n",
    "    plot_dataset(X, Y, ax)\n",
    "    plot_separator(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        sklearn.svm.SVC(C = C, kernel = 'poly', degree=gamma).fit(X, Y),\n",
    "        cmap=plt.cm.viridis,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    ax.set_title('Kernel polynômial, $\\gamma$ = ' + str(gamma))\n",
    "\n",
    "\n",
    "gammas = [.01, .1, 2, 10, 100, 1000]\n",
    "for gamma, ax in zip(gammas, list(axes[1, :])):\n",
    "    plot_dataset(X, Y, ax)\n",
    "    plot_separator(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        sklearn.svm.SVC(C = C, kernel = 'rbf', gamma=gamma).fit(X, Y),\n",
    "        cmap=plt.cm.viridis,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    ax.set_title('Kernel rbf, $\\gamma$ = ' + str(gamma))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réponse:\n",
    "\n",
    "6.\n",
    "b.\n",
    "Dans ce cas, les deux classes ne sont pas séparables linéairement (i.e. Kernel pôlynomial avec ). On applique donc deux types de transformation d'espace.\n",
    "\n",
    "* le kernel pôlynomial: On remarque ici deux comportements: le cas où le degré du polynôme est paire, et donc la transformation est symétrique par rotation, et le deuxième cas où le degré est impaire, et, en conséquence, la transformation est asymétrique. Vue la symmétrie de la donnée d'entraînement, les transformations asymétrique ne correspondent pas au problème posé. Concernant les degrés paires, le cas de $\\gamma = 2$ semble marcher le mieux. En effet, plus le degré est grand, plus on pénalise les grandes distances. Il faut donc adapter le degré à la distribution de la donnée: i.e. différencier entre les points rouges et bleus qui sont relativement loin, sans pénaliser non plus les distances entre les points propres à la même classe. Effectivement, à partir du degré $\\gamma = 4$ les distances entre les points bleus qui sont diamétralement opposés sont confondues avec les points rouges les plus proches. C'est ainsi que les points bleus sont confondus, dans la même classe, avec les points rouges les plus proches.\n",
    "* le kernel rbf: On rappelle la formule de ce kernel: \n",
    "$$\\forall x,y \\in \\mathbb{R}^d \\quad k_{\\gamma}(x,y) = e^{-\\gamma.\\vert\\vert x-y\\vert\\vert^2}$$ On remarque donc que quand $\\gamma$ augmente, pour une distance $\\vert\\vert x-y\\vert\\vert$ fixée, $k_{\\gamma}(x,y)$ est plus faible. On peut même remarquer que: $$\\forall x, y \\in \\mathbb{R}^d \\quad \\sqrt{\\frac{\\gamma}{\\pi}}.k_{\\gamma}(x, y) \\xrightarrow[\\gamma \\rightarrow \\infty]{} \\delta(\\vert\\vert x-y\\vert\\vert)$$\n",
    "On remarque donc que, quand $\\gamma$ grandit la ligne de séparation colle de plus en plus sur les points rouges jusqu'à donner la fonction de décision naïve $D_n \\triangleq 2.\\sum_{i \\in \\{y^i > 0\\}} \\delta_{x^i} - 1$. Il y a donc un compromis à trouver entre précision de classification et généralisation à travers le choix du $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation croisée\n",
    "\n",
    "### Train-Test split\n",
    "\n",
    "Afin d'estimer le pouvoir de généralisation d'un classifieur, il faut le tester sur de nouvelles instances. On parle de données d'entraînement et données de tests. En pratique, on garde aussi des données de côtés pour la validation après calibrage entre entraînement et tests.\n",
    "\n",
    "1. a. En utilisant la fonction [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) fournie par scikit-learn, entraîner un SVM linéaire sur 80% de vos données et tester sur le reste.\n",
    "\n",
    "   b. Répéter l'experience plusieurs fois. Commenter les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "\n",
    "X, Y = sklearn.datasets.make_blobs(n_samples=200, centers=2, random_state=0, cluster_std=1.5)\n",
    "\n",
    "# Répartir les données en 4/5 de train data et 1/5 de test data\n",
    "X_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split( X, Y, test_size=.2, random_state=42)\n",
    "\n",
    "# Entraîner le modèle\n",
    "classifier = sklearn.svm.SVC(kernel='linear', C=1).fit(X_train, Y_train)\n",
    "\n",
    "# Tester le modèle entraîné\n",
    "test_score = classifier.score(X_test, Y_test)\n",
    "\n",
    "print('Test score :', test_score)\n",
    "\n",
    "f, ax = plt.subplots(1, 1)\n",
    "f.set_figheight(10)\n",
    "f.set_figwidth(10)\n",
    "\n",
    "plot_dataset(X_train, Y_train, ax)\n",
    "plot_dataset(X_test, Y_test, ax, ['y', 'g'])\n",
    "xx, yy = mesh_from(X, .01)\n",
    "plot_margin(\n",
    "    xx,\n",
    "    yy,\n",
    "    ax,\n",
    "    classifier\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recherche de paramètres\n",
    "\n",
    "L'idée de la validation croisée et que l'on varie les données d'entraînement et de test, de façon à ne pas entraîner sur les mauvaises instances et puis tester sur les instances les plus durs.\n",
    "\n",
    "On subdivise donc toutes les données en $K$ parts égales. A l'instant $k = 1,\\dots,K$, on isole la $k^{ième}$ part comme ensemble de test et on entraîne notre modèle sur les $K -1$ parties restantes. On obtient donc, $K$ score de test. Dans le meilleur des cas, on tombe sur les instances qui donnent le plus de pouvoir de généralisation possible.\n",
    "\n",
    "Pour le SVM, avec juste les vecteurs supports, ce qui reprèsente moins de $10\\%$ de la donnée dans notre cas, on obtient le meilleur séparateur linéaire. En cas pratique, au moment de la validation, on ne connaît pas les instances à prédire. On n'est pas sûr donc de tomber sur les vecteurs supports du meilleur modèle qui résoud le problème. On cherche donc, grâce à la validation croisée, les points les plus proches de la marge; pour avoir ainsi, le meilleur pouvoir de généralisation.\n",
    "\n",
    "La généralisation passe aussi par le bon choix des paramètres du modèle. On utilise donc cette approche dans le but de trouver expérimentalement les meilleurs paramètres. Aussi, répète-t-on l'expérience afin d'essayer autant de configurations possibles. Les paramètres qui donnent les meilleurs scores de tests seront choisis au bout de l'étude.\n",
    "\n",
    "Le *test score* n'est pas la seule métrique possible. On peut chercher à maximiser le *F-score*, comme on peut aussi s'intéresser qu'au score d'une classe donnée:\n",
    "\n",
    "* Exemple: Vaudrait mieux un faux signal positif au scanner de bagage qu'un faux négatif (i.e. drogue ou explosif détectés comme sûrs ou un test sangui négatif pour un patient malade).\n",
    "\n",
    "\n",
    "7.\n",
    "    En utilisant la fonction [cross_validate](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) de scikit-learn, trouver la bonne valeur de $C$ pour un modèle SVM linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_metrics(scores):\n",
    "    return  (\n",
    "                np.max(scores),\n",
    "                np.mean(scores),\n",
    "                np.median(scores),\n",
    "                np.min(scores)\n",
    "            )\n",
    "\n",
    "Cs = [pow(2, p) for p in range(-15, 15)]\n",
    "\n",
    "# tester avec tout les C dans Cs est stocker les scores\n",
    "cv_scores = [\n",
    "        sklearn.model_selection.cross_validate(\n",
    "                sklearn.svm.SVC(kernel='linear', C=C),\n",
    "                X,\n",
    "                Y,\n",
    "                scoring='accuracy',\n",
    "                cv=5\n",
    "        )\n",
    "        for C in Cs\n",
    "    ]\n",
    "test_scores = [cv_metrics(scores['test_score']) for scores in cv_scores]\n",
    "\n",
    "# le meilleur C est:\n",
    "test_score = max(test_scores)\n",
    "\n",
    "C = Cs[\n",
    "    test_scores.index(test_score)\n",
    "]\n",
    "print('Le meilleur paramètre de pénalisation des variables ressort est :', C, ', avec un test score moyen de :', test_score[1])\n",
    "\n",
    "print('Ce modèle donne au moment de l\\'entraînement, un score moyen de :', np.mean(cv_scores[test_scores.index(test_score)]['train_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commentaire:\n",
    "\n",
    "On peut remarquer le fait que le score moyen en test est sensiblement plus grand que le score d'entraînement. Ceci est dû surtout au fait que les point d'entraînement étaient plus dûrs et que l'on gagne en généralisation avec le bon choix de $C$. On remarque aussi qu'avec la validation croisée, on obtient bien un meilleur résultat que celui de l'expérience précédente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM vs Random Forest\n",
    "\n",
    "8.\n",
    "   a. Comparer le meilleur kernel SVM trouver dans la section 'Kernel SVM' avec une forêt aléatoire de votre choix. \n",
    "\n",
    "   b. Tracer les courbes de séparation.\n",
    "\n",
    "   c. Justifier votre choix de nombre d'arbres et de profondeur.\n",
    "\n",
    "9.\n",
    "    Commenter les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "import sklearn.tree\n",
    "\n",
    "X, Y = sklearn.datasets.make_circles(n_samples=100, factor=.3, noise=.05)\n",
    "xx, yy = mesh_from(X, .01)\n",
    "\n",
    "models = [\n",
    "    sklearn.svm.SVC(kernel='poly', degree=2),\n",
    "    sklearn.svm.SVC(kernel='rbf', gamma=2),\n",
    "    sklearn.tree.DecisionTreeClassifier(),\n",
    "    sklearn.ensemble.RandomForestClassifier(n_estimators=1000, max_depth=1),\n",
    "    sklearn.ensemble.RandomForestClassifier(n_estimators=1000, max_depth=2),\n",
    "    sklearn.ensemble.RandomForestClassifier(n_estimators=1000, max_depth=4),\n",
    "    sklearn.ensemble.RandomForestClassifier(n_estimators=1000)\n",
    "]\n",
    "\n",
    "titles=[\n",
    "    'Quadratic polynomial kernel SVM',\n",
    "    'RBF kernel SVM with $\\gamma = 2$',\n",
    "    'Decision Tree, i.e. n_trees = 1',\n",
    "    'Random Forest, n_trees = 1000, max_depth = 1',\n",
    "    'Random Forest, n_trees = 1000, max_depth = 2',\n",
    "    'Random Forest, n_trees = 1000, max_depth = 4',\n",
    "    'Random Forest, n_trees = 1000'\n",
    "]\n",
    "\n",
    "f, axes = plt.subplots(1, len(models))\n",
    "f.set_figheight(10)\n",
    "f.set_figwidth(10*len(models))\n",
    "\n",
    "for model, ax, title in zip(models, axes, titles):\n",
    "    plot_dataset(X, Y, ax)\n",
    "    model.fit(X, Y)\n",
    "    plot_separator(\n",
    "        xx,\n",
    "        yy,\n",
    "        ax,\n",
    "        model,\n",
    "        cmap=plt.cm.viridis,\n",
    "        alpha=0.5\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réponse\n",
    "\n",
    "8.\n",
    "La forêt aléatoire arrive à bien séparer les deux classes. Le nombre d'estimateurs doit bien évidemment être très grand. La profondeur doit être du même ordre de grandeur que la dimension de la donnée ($d=2$). On remarque bien que la forêt aléatoire avec $1000$ arbres et $2$ en profondeur arrive bien à séparer la donnée. Rajoutter de la profondeur ne rajoutte pas vraiment de pouvoir de généralisation pour le problème.\n",
    "\n",
    "9.\n",
    "Le SVM arrive à donner la séparation avec le plus de marge possible. En pratique, on a pas toujours le privilège de connaître à l'avance la géométrie de la données. Il est plus facile d'entraîner des forêt alétoires que de trouver le kernel le mieux adapté."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection d'attribut\n",
    "\n",
    "### Occupation des sols\n",
    "\n",
    "L'occupation des sols à pour but de donner pour le type d'usage faits des terres. Naturellement, la manière la moins couteuse pour obtenir, à large échelle et à très grande fréquence, cette information, serait une approche automatique basée sur les images satellitaires.\n",
    "\n",
    "On cherche à assigner, pour chaque pixel, un des types d'usage possibles, en utilisant la valeur du pixel ou son voisinage. On modèlise donc le problème avec classification supervisée.\n",
    "\n",
    "#### Présentation de la donnée\n",
    "\n",
    "Pour ce TP nous utilisons une image du satellite optique [Sentinel-2 du programme européen Copernicus](http://www.esa.int/Our_Activities/Observing_the_Earth/Copernicus/Sentinel-2). Cette image est acquise le 10 juillet 2016 et téléchargée depuis la plateforme [Theia](https://theia.cnes.fr).\n",
    "\n",
    "10, des 13 bandes spectrales du satellite Sentinel-2, y sont disponibles en niveau de traitement 2A: B2, B3, B4, B5, B6, B7, B8, B8A, B11 et B12. Ces 10 bandes spectrales ont été réchantillonnées en géométrie terrain (Lambert 93) à 10 m de résolution spatiale et assemblées dans le fichier `sentinel-2_sample.tif`.\n",
    "\n",
    "Les bandes spectrales de *Sentinel 2*:\n",
    "![Les bandes spectrales de *Sentinel 2*][sentinel_2]\n",
    "\n",
    "L'image `sentinel-2_sample.tif` concerne une zone de $14$ Km $\\times14$ Km dans le département de la Haute-Garonne (31): ville de Saint-Gaudens. Toutes les données se trouvent dans le répértoire `./data`.\n",
    "\n",
    "On dispose aussi de :\n",
    "* `RGE-OCS.shp` : un extrait de l’OCS GE (**OC**cupation du **S**ol **G**rande **E**chelle) de l’IGN sur la zone d’étude;\n",
    "* `RGE-foret.shp` : un extrait de la BD Forêt de l’IGN sur la zone d’étude.\n",
    "\n",
    "A partir de ces données, on obtient la vérité terrain *raster* à la même échelle pour chaque pixel dans:\n",
    "* `ground_truth_landcover.tif`: vérité terrain OCS générale.\n",
    "* `ground_truth_forest.tif`: vérité terrain raster forêt-non forêt.\n",
    "\n",
    "\n",
    "1. a. Ouvrir le fichier projet `dataset.qgs` avec QGIS. \n",
    "\n",
    "   b. Etudier l'histogramme des bandes de l'image hyperspectrale et la vérité terrain.\n",
    "   \n",
    "2. a. Charger l'image sur python en se servant de *gdal*.\n",
    "\n",
    "   b. Ajouter le *NDVI* comme bande supplémentaire à votre donnée.\n",
    "   \n",
    "     * Rappel: $$NDVI = \\frac{{\\text{NIR}}-{\\text{Red}}}{{\\text{NIR}}+{\\text{Red}}}$$\n",
    "     * Astuce: ajouter un $\\epsilon$ pour ne pas diviser sur zéro. $$NDVI = \\frac{{\\text{NIR}}-{\\text{Red}}}{{\\text{NIR}}+{\\text{Red}}+\\epsilon}$$\n",
    "\n",
    "   c. Séparer les pixels en données d'entraînement et données de validation à un ratio de 4/5.\n",
    "   \n",
    "   d. Utiliser la validation croisée pour trouver le meilleur kernel et les meilleurs paramètres de votre SVM.\n",
    "   \n",
    "   e. Qualifier les résultats obtenus.\n",
    "\n",
    "[sentinel_2]: http://www.cesbio.ups-tlse.fr/data_all/images/sentinel1.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal\n",
    "import gdalconst\n",
    "import itertools\n",
    "\n",
    "def read(filename, ndtype=np.float64):\n",
    "    \"\"\"\n",
    "        reads all bands of raster images.\n",
    "        \n",
    "        :param filename: the path to the raster image\n",
    "        :type filename: string\n",
    "        :return: a list containing a numpy matrix for each band\n",
    "        :rtype: list\n",
    "    \"\"\"\n",
    "    dataset = gdal.Open(filename, gdalconst.GA_ReadOnly)\n",
    "    return [\n",
    "        dataset.GetRasterBand(band).ReadAsArray().astype(ndtype)\n",
    "        for band in range(1, dataset.RasterCount + 1)\n",
    "    ]\n",
    "\n",
    "\n",
    "def add_band(image, lhs, rhs, func):\n",
    "    \"\"\"\n",
    "        add a band to an image by applying a function on two of the image bands.\n",
    "        \n",
    "        :param image: a list containing the image bands.\n",
    "        :param lhs: index of the first band to use\n",
    "        :param rhs: index of the second band to use\n",
    "        :param func: the function to apply on the two bands\n",
    "        :type image: list\n",
    "        :type lhs: int\n",
    "        :type rhs: int\n",
    "        :type func: function\n",
    "        :return: a list containing a numpy matrix for each band\n",
    "        :rtype: list\n",
    "    \"\"\"\n",
    "    image.append(func(image[lhs], image[rhs]))\n",
    "\n",
    "\n",
    "def add_ndvi(image, epsilon = 1E-6):\n",
    "    add_band(image, 2, 3, lambda x, y: (y-x)/(x + y + epsilon))\n",
    "\n",
    "\n",
    "band_names = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11' , 'B12']\n",
    "bands = read('./data/sentinel-2_sample.tif')\n",
    "\n",
    "add_ndvi(bands)\n",
    "\n",
    "# Flatten pixels\n",
    "X = np.array(list(zip(*[band.flatten() for band in bands])))\n",
    "\n",
    "# Load ground truth\n",
    "Y = (read('./data/ground_truth_forest.tif', np.int8)[0] > 1).flatten()\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = sklearn.model_selection.train_test_split(X, Y, test_size=.8, random_state=42)\n",
    "\n",
    "Cs = [pow(10, p) for p in range(-2, 1)]\n",
    "polynomial_svms = [('Polynomial', gamma, sklearn.svm.SVC(kernel='poly', degree=gamma)) for gamma in  range(1, 3)]\n",
    "rbf_svms = [('RBF', gamma, sklearn.svm.SVC(kernel='rbf', gamma=gamma)) for gamma in  [pow(10, p) for p in range(-2, 1)]]\n",
    "\n",
    "\n",
    "cv_scores = [\n",
    "    (C, kernel_type, gamma, model, sklearn.model_selection.cross_validate(model, X_train, Y_train, scoring='accuracy', cv=5, n_jobs=-1))\n",
    "    for (C, (kernel_type, gamma, model)) in list(itertools.product(Cs, polynomial_svms + rbf_svms))[:1]\n",
    "]\n",
    "\n",
    "test_scores = [(C, kernel_type, gamma, model, cv_metrics(scores['test_score'])) for (C, kernel_type, gamma, model, scores) in cv_scores]\n",
    "\n",
    "C, kernel_type, gamma, max_model, max_score = max(test_scores, lambda x: x[4])\n",
    "\n",
    "print('La meilleur combinaison correspond à: C =', C, 'avec un kernel', kernel_type, 'et \\gamma = ', gamma, 'qui donne un test score de:', max_score, 'et une score de validation:', max_model.score(X_validation, Y_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sélections d'attributs:\n",
    "\n",
    "1. a. Estimer le nombre de toutes combinaisons d'attributs possibles.\n",
    "\n",
    "   b. En utilisant les méthodes vues au cours (SVM-RFE, SFS , BFS et LR), établir une hiérarchie d'attributs (i.e. des bandes).\n",
    "2. Comparer les différentes méthodes.\n",
    "3. Commenter les hiérarchies obtenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_selection\n",
    "\n",
    "\n",
    "def try_add_attribute(selected, idx, X, Y, classifier):\n",
    "    return sklearn.model_selection.cross_validate(\n",
    "        classifier,\n",
    "        X[:, list(selected) + [idx]],\n",
    "        Y,\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )['test_score']\n",
    "\n",
    "\n",
    "def try_remove_attribute(selected, idx, X, Y, classifier):\n",
    "    trying = set(selected)\n",
    "    trying.remove(idx)\n",
    "    return sklearn.model_selection.cross_validate(\n",
    "        classifier,\n",
    "        X[:, list(trying)],\n",
    "        Y,\n",
    "        scoring='accuracy',\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )['test_score']\n",
    "\n",
    "\n",
    "def add_best_attribute(selected, X, Y, classifier):\n",
    "    max_idx, _ = max(\n",
    "        [\n",
    "            (\n",
    "                idx,\n",
    "                cv_metrics(\n",
    "                    try_attribute(selected, idx, X, Y, classifier)\n",
    "                )\n",
    "            )\n",
    "            for idx in (set(range(X.shape[1])) - selected)\n",
    "        ],\n",
    "        lambda x: x[1]\n",
    "    )\n",
    "    selected.add(max_idx)\n",
    "    return selected\n",
    "\n",
    "\n",
    "def remove_best_attribute(selected, X, Y, classifier):\n",
    "    max_idx, _ = max(\n",
    "        [\n",
    "            (\n",
    "                idx,\n",
    "                cv_metrics(\n",
    "                    try_attribute(selected, idx, X, Y, classifier)\n",
    "                )\n",
    "            )\n",
    "            for idx in selected\n",
    "        ],\n",
    "        lambda x: x[1]\n",
    "    )\n",
    "    selected.remove(max_idx)\n",
    "    return selected\n",
    "\n",
    "\n",
    "def add_best_L_attributes(selected, L, X, Y, classifier):\n",
    "    if L + len(selected) >= X.shape[1]:\n",
    "        return set(range(X.shape[1]))\n",
    "    else:\n",
    "        for i in range(L):\n",
    "            selected = add_best_attribute(selected, X, Y, classifier)\n",
    "        return selected\n",
    "\n",
    "\n",
    "def remove_worst_R_attributes(selected, R, X, Y, classifier):\n",
    "    if len(selected) - R <= 0:\n",
    "        return set()\n",
    "    else:\n",
    "        for i in range(min(R, len(selected))):\n",
    "            selcted = remove_best_attribute(selected, X, Y, classifier)\n",
    "        return selected\n",
    "\n",
    "\n",
    "def sfs(X, Y, number_of_attributes, classifier):\n",
    "    return add_best_L_attributes(set(), number_of_attributes, X, Y, classifier)\n",
    "\n",
    "\n",
    "def bfs(X, Y, number_of_attributes, classifier):\n",
    "    return remove_worst_R_attributes(set(range(X.shape[1])), X.shape[1] - number_of_attributes, X, Y, classifier)\n",
    "\n",
    "def lr(X, Y, number_of_attributes, L, R, classifier):\n",
    "    selected = set()\n",
    "    if L > R:\n",
    "        selected = add_best_L_attributes(selected, L, X, Y, classifier)\n",
    "    else:\n",
    "        selected = set(range(X.shape[1]))\n",
    "        selected = remove_worst_R_attributes(selected, R, X, Y, classifier)\n",
    "    while len(selected) != number_attributes:\n",
    "        if L > R:\n",
    "            selected = remove_worst_R_attributes(selected, L, X, Y, classifier)\n",
    "        else:\n",
    "            selected = add_best_L_attributes(selected, R, X, Y, classifier)\n",
    "    return selected\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}